"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7856],{7601:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>p,frontMatter:()=>l,metadata:()=>o,toc:()=>c});var s=t(4848),i=t(8453);const l={},a="vLLM Inference",o={id:"installation/advanced_usage/vLLM_inference",title:"vLLM Inference",description:"DB-GPT supports vLLM inference, a fast and easy-to-use LLM inference and service library.",source:"@site/docs/installation/advanced_usage/vLLM_inference.md",sourceDirName:"installation/advanced_usage",slug:"/installation/advanced_usage/vLLM_inference",permalink:"/docs/latest/installation/advanced_usage/vLLM_inference",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"ProxyLLMs",permalink:"/docs/latest/installation/advanced_usage/More_proxyllms"},next:{title:"OpenAI SDK Calls Local Multi-model",permalink:"/docs/latest/installation/advanced_usage/OpenAI_SDK_call"}},r={},c=[{value:"Install dependencies",id:"install-dependencies",level:2},{value:"Modify configuration file",id:"modify-configuration-file",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"vllm-inference",children:"vLLM Inference"}),"\n",(0,s.jsxs)(n.p,{children:["DB-GPT supports ",(0,s.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," inference, a fast and easy-to-use LLM inference and service library."]}),"\n",(0,s.jsx)(n.h2,{id:"install-dependencies",children:"Install dependencies"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"vLLM"})," is an optional dependency in DB-GPT. You can install it manually through the following command."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'$ pip install -e ".[vllm]"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"modify-configuration-file",children:"Modify configuration file"}),"\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.code,{children:".env"})," configuration file, modify the inference type of the model to start ",(0,s.jsx)(n.code,{children:"vllm"})," inference."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"LLM_MODEL=vicuna-13b-v1.5\nMODEL_TYPE=vllm\n"})}),"\n",(0,s.jsxs)(n.p,{children:["For more information about the list of models supported by ",(0,s.jsx)(n.code,{children:"vLLM"}),", please refer to the ",(0,s.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models",children:"vLLM supported model document"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const i={},l=s.createContext(i);function a(e){const n=s.useContext(l);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);