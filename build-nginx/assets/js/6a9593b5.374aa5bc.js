"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6849],{251:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var s=n(4848),i=n(8453);const o={},r="LLM USE FAQ",a={id:"faq/llm",title:"LLM USE FAQ",description:"Q1:how to use openai chatgpt service",source:"@site/versioned_docs/version-v0.5.0/faq/llm.md",sourceDirName:"faq",slug:"/faq/llm",permalink:"/docs/v0.5.0/faq/llm",draft:!1,unlisted:!1,tags:[],version:"v0.5.0",frontMatter:{},sidebar:"docs",previous:{title:"Installation FAQ",permalink:"/docs/v0.5.0/faq/install"},next:{title:"KBQA FAQ",permalink:"/docs/v0.5.0/faq/kbqa"}},l={},c=[{value:"Q1 to use openai chatgpt service",id:"q1-to-use-openai-chatgpt-service",level:3},{value:"Q2 What difference between <code>python dbgpt_server --light</code> and <code>python dbgpt_server</code>",id:"q2-what-difference-between-python-dbgpt_server---light-and-python-dbgpt_server",level:3},{value:"Q3 how to use MultiGPUs",id:"q3-how-to-use-multigpus",level:3},{value:"Q4 Not Enough Memory",id:"q4-not-enough-memory",level:3}];function d(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h3:"h3",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"llm-use-faq",children:"LLM USE FAQ"}),"\n",(0,s.jsxs)(t.h3,{id:"q1-to-use-openai-chatgpt-service",children:["Q1",":how"," to use openai chatgpt service"]}),"\n",(0,s.jsx)(t.p,{children:"change your LLM_MODEL"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-shell",children:"LLM_MODEL=proxyllm\n"})}),"\n",(0,s.jsx)(t.p,{children:"set your OPENAPI KEY"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-shell",children:"PROXY_API_KEY={your-openai-sk}\nPROXY_SERVER_URL=https://api.openai.com/v1/chat/completions\n"})}),"\n",(0,s.jsx)(t.p,{children:"make sure your openapi API_KEY is available"}),"\n",(0,s.jsxs)(t.h3,{id:"q2-what-difference-between-python-dbgpt_server---light-and-python-dbgpt_server",children:["Q2 What difference between ",(0,s.jsx)(t.code,{children:"python dbgpt_server --light"})," and ",(0,s.jsx)(t.code,{children:"python dbgpt_server"})]}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsxs)(t.p,{children:["python dbgpt_server --light",(0,s.jsx)(t.code,{children:"dbgpt_server does not start the llm service. Users can deploy the llm service separately by using"}),"python llmserver`, and dbgpt_server accesses the llm service through set the LLM_SERVER environment variable in .env. The purpose is to allow for the separate deployment of dbgpt's backend service and llm service."]}),(0,s.jsx)(t.p,{children:"python dbgpt_server service and the llm service are deployed on the same instance. when dbgpt_server starts the service, it also starts the llm service at the same time."})]}),"\n",(0,s.jsx)(t.h3,{id:"q3-how-to-use-multigpus",children:"Q3 how to use MultiGPUs"}),"\n",(0,s.jsxs)(t.p,{children:["DB-GPT will use all available gpu by default. And you can modify the setting ",(0,s.jsx)(t.code,{children:"CUDA_VISIBLE_DEVICES=0,1"})," in ",(0,s.jsx)(t.code,{children:".env"})," file\nto use the specific gpu IDs."]}),"\n",(0,s.jsx)(t.p,{children:"Optionally, you can also specify the gpu ID to use before the starting command, as shown below:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-shell",children:"# Specify 1 gpu\nCUDA_VISIBLE_DEVICES=0 python3 dbgpt/app/dbgpt_server.py\n\n# Specify 4 gpus\nCUDA_VISIBLE_DEVICES=3,4,5,6 python3 dbgpt/app/dbgpt_server.py\n"})}),"\n",(0,s.jsxs)(t.p,{children:["You can modify the setting ",(0,s.jsx)(t.code,{children:"MAX_GPU_MEMORY=xxGib"})," in ",(0,s.jsx)(t.code,{children:".env"})," file to configure the maximum memory used by each GPU."]}),"\n",(0,s.jsx)(t.h3,{id:"q4-not-enough-memory",children:"Q4 Not Enough Memory"}),"\n",(0,s.jsx)(t.p,{children:"DB-GPT supported 8-bit quantization and 4-bit quantization."}),"\n",(0,s.jsxs)(t.p,{children:["You can modify the setting ",(0,s.jsx)(t.code,{children:"QUANTIZE_8bit=True"})," or ",(0,s.jsx)(t.code,{children:"QUANTIZE_4bit=True"})," in ",(0,s.jsx)(t.code,{children:".env"})," file to use quantization(8-bit quantization is enabled by default)."]}),"\n",(0,s.jsx)(t.p,{children:"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit quantization can run with 48 GB of VRAM."}),"\n",(0,s.jsxs)(t.p,{children:["Note: you need to install the latest dependencies according to ",(0,s.jsx)(t.a,{href:"https://github.com/eosphoros-ai/DB-GPT/blob/main/requirements.txt",children:"requirements.txt"}),".\nNote: you need to install the latest dependencies according to ",(0,s.jsx)(t.a,{href:"https://github.com/eosphoros-ai/DB-GPT/blob/main/requirements.txt",children:"requirements.txt"}),"."]})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>a});var s=n(6540);const i={},o=s.createContext(i);function r(e){const t=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:t},e.children)}}}]);