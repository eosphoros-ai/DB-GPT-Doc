"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6643],{8825:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>d,toc:()=>r});var t=o(4848),l=o(8453);const a={},i="Stand-alone Deployment",d={id:"installation/model_service/stand_alone",title:"Stand-alone Deployment",description:"Preparation",source:"@site/docs/installation/model_service/stand_alone.md",sourceDirName:"installation/model_service",slug:"/installation/model_service/stand_alone",permalink:"/docs/latest/installation/model_service/stand_alone",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Docker-Compose Deployment",permalink:"/docs/latest/installation/docker_compose"},next:{title:"Cluster Deployment",permalink:"/docs/latest/installation/model_service/cluster"}},s={},r=[{value:"Preparation",id:"preparation",level:2},{value:"Environment installation",id:"environment-installation",level:2},{value:"Install dependencies",id:"install-dependencies",level:2},{value:"Model download",id:"model-download",level:2},{value:"Command line startup",id:"command-line-startup",level:2},{value:"View and verify model serving",id:"view-and-verify-model-serving",level:2}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"stand-alone-deployment",children:"Stand-alone Deployment"}),"\n",(0,t.jsx)(n.h2,{id:"preparation",children:"Preparation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# download source code\n$ git clone https://github.com/eosphoros-ai/DB-GPT.git\n\n$ cd DB-GPT\n"})}),"\n",(0,t.jsx)(n.h2,{id:"environment-installation",children:"Environment installation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# create a virtual environment\n$ conda create -n dbgpt_env python=3.10\n\n# activate virtual environment\n$ conda activate dbgpt_env\n"})}),"\n",(0,t.jsx)(n.h2,{id:"install-dependencies",children:"Install dependencies"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pip install -e ".[default]"\n'})}),"\n",(0,t.jsx)(n.h2,{id:"model-download",children:"Model download"}),"\n",(0,t.jsx)(n.p,{children:"Download LLM and Embedding model"}),"\n",(0,t.jsx)(n.admonition,{title:"note",type:"info",children:(0,t.jsx)(n.p,{children:"\u26a0\ufe0f If there are no GPU resources, it is recommended to use the proxy model, such as OpenAI, Qwen, ERNIE Bot, etc."})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"$ mkdir models && cd models\n\n# download embedding model, eg: text2vec-large-chinese\n$ git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Set up proxy API and modify ",(0,t.jsx)(n.code,{children:".env"}),"configuration"]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#set LLM_MODEL TYPE\nLLM_MODEL=proxyllm\n#set your Proxy Api key and Proxy Server url\nPROXY_API_KEY={your-openai-sk}\nPROXY_SERVER_URL=https://api.openai.com/v1/chat/completions\n"})}),"\n",(0,t.jsx)(n.admonition,{title:"note",type:"info",children:(0,t.jsx)(n.p,{children:"\u26a0\ufe0f If you have GPU resources, you can use local models to deploy"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"$ mkdir models && cd models\n\n# # download embedding model, eg: vicuna-13b-v1.5 or  \n$ git clone https://huggingface.co/lmsys/vicuna-13b-v1.5\n\n# download embedding model, eg: text2vec-large-chinese\n$ git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\n\n$ popd\n\n"})}),"\n",(0,t.jsx)(n.h2,{id:"command-line-startup",children:"Command line startup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"LLM_MODEL=vicuna-13b-v1.5 \ndbgpt start webserver --port 6006\n"})}),"\n",(0,t.jsxs)(n.p,{children:["By default, the ",(0,t.jsx)(n.code,{children:"dbgpt start webserver command"})," will start the ",(0,t.jsx)(n.code,{children:"webserver"}),", ",(0,t.jsx)(n.code,{children:"model controller"}),", and ",(0,t.jsx)(n.code,{children:"model worker"})," through a single Python process. In the above command, port ",(0,t.jsx)(n.code,{children:"6006"})," is specified."]}),"\n",(0,t.jsx)(n.h2,{id:"view-and-verify-model-serving",children:"View and verify model serving"}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"view and display all model services"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"dbgpt model list \n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# result\n+-----------------+------------+------------+------+---------+---------+-----------------+----------------------------+\n|    Model Name   | Model Type |    Host    | Port | Healthy | Enabled | Prompt Template |       Last Heartbeat       |\n+-----------------+------------+------------+------+---------+---------+-----------------+----------------------------+\n| vicuna-13b-v1.5 |    llm     | 172.17.0.9 | 6006 |   True  |   True  |                 | 2023-10-16T19:49:59.201313 |\n|  WorkerManager  |  service   | 172.17.0.9 | 6006 |   True  |   True  |                 | 2023-10-16T19:49:59.246756 |\n+-----------------+------------+------------+------+---------+---------+-----------------+----------------------------+\n\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Where ",(0,t.jsx)(n.code,{children:"WorkerManager"})," is the management process of ",(0,t.jsx)(n.code,{children:"Model Workers"})]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"check and verify model serving"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"dbgpt model chat --model_name vicuna-13b-v1.5\n"})}),"\n",(0,t.jsx)(n.p,{children:"The above command will launch an interactive page that allows you to talk to the model through the terminal."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"Chatbot started with model vicuna-13b-v1.5. Type 'exit' to leave the chat.\n\n\nYou: Hello\nBot: Hello! How can I assist you today?\n\nYou: \n"})})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>d});var t=o(6540);const l={},a=t.createContext(l);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);