"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1645],{5704:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>m,frontMatter:()=>l,metadata:()=>d,toc:()=>i});var o=t(4848),r=t(8453);const l={},a="Cluster Deployment",d={id:"installation/model_service/cluster",title:"Cluster Deployment",description:"Install command line tools",source:"@site/versioned_docs/version-v0.5.0/installation/model_service/cluster.md",sourceDirName:"installation/model_service",slug:"/installation/model_service/cluster",permalink:"/docs/v0.5.0/installation/model_service/cluster",draft:!1,unlisted:!1,tags:[],version:"v0.5.0",frontMatter:{},sidebar:"docs",previous:{title:"Stand-alone Deployment",permalink:"/docs/v0.5.0/installation/model_service/stand_alone"},next:{title:"ProxyLLMs",permalink:"/docs/v0.5.0/installation/advanced_usage/More_proxyllms"}},s={},i=[{value:"Install command line tools",id:"install-command-line-tools",level:2},{value:"Start Model Controller",id:"start-model-controller",level:2},{value:"View log",id:"view-log",level:2},{value:"Start Model Worker",id:"start-model-worker",level:2},{value:"Start the embedding model service",id:"start-the-embedding-model-service",level:2},{value:"Use model serving",id:"use-model-serving",level:2},{value:"Start Webserver",id:"start-webserver",level:2},{value:"Command line usage",id:"command-line-usage",level:2}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"cluster-deployment",children:"Cluster Deployment"}),"\n",(0,o.jsx)(n.h2,{id:"install-command-line-tools",children:"Install command line tools"}),"\n",(0,o.jsxs)(n.p,{children:["All the following operations are completed through the ",(0,o.jsx)(n.code,{children:"dbgpt"})," command. To use the ",(0,o.jsx)(n.code,{children:"dbgpt"})," command, you first need to install the ",(0,o.jsx)(n.code,{children:"DB-GPT"})," project. You can install it through the following command"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'$ pip install -e ".[default]"\n'})}),"\n",(0,o.jsx)(n.p,{children:"It can also be used in script mode"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"$ python pilot/scripts/cli_scripts.py\n"})}),"\n",(0,o.jsx)(n.h2,{id:"start-model-controller",children:"Start Model Controller"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"$ dbgpt start controller\n"})}),"\n",(0,o.jsx)(n.h2,{id:"view-log",children:"View log"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"$ docker logs db-gpt-webserver-1 -f\n"})}),"\n",(0,o.jsxs)(n.p,{children:["By default, ",(0,o.jsx)(n.code,{children:"Model Server"})," will start on port ",(0,o.jsx)(n.code,{children:"8000"})]}),"\n",(0,o.jsx)(n.h2,{id:"start-model-worker",children:"Start Model Worker"}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["Start ",(0,o.jsx)(n.code,{children:"chatglm2-6b"})," model Worker"]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"dbgpt start worker --model_name chatglm2-6b \\\n--model_path /app/models/chatglm2-6b \\\n--port 8001 \\\n--controller_addr http://127.0.0.1:8000\n"})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["Start ",(0,o.jsx)(n.code,{children:"vicuna-13b-v1.5"})," model Worker"]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"dbgpt start worker --model_name vicuna-13b-v1.5 \\\n--model_path /app/models/vicuna-13b-v1.5 \\\n--port 8002 \\\n--controller_addr http://127.0.0.1:8000\n"})}),"\n",(0,o.jsx)(n.admonition,{title:"note",type:"info",children:(0,o.jsx)(n.p,{children:"\u26a0\ufe0f  Make sure to use your own model name and model path."})}),"\n",(0,o.jsx)(n.h2,{id:"start-the-embedding-model-service",children:"Start the embedding model service"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"dbgpt start worker --model_name text2vec \\\n--model_path /app/models/text2vec-large-chinese \\\n--worker_type text2vec \\\n--port 8003 \\\n--controller_addr http://127.0.0.1:8000\n"})}),"\n",(0,o.jsx)(n.admonition,{title:"note",type:"info",children:(0,o.jsx)(n.p,{children:"\u26a0\ufe0f  Make sure to use your own model name and model path."})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsx)(n.p,{children:"View and inspect deployed models"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"$ dbgpt model list\n\n+-----------------+------------+------------+------+---------+---------+-----------------+----------------------------+\n|    Model Name   | Model Type |    Host    | Port | Healthy | Enabled | Prompt Template |       Last Heartbeat       |\n+-----------------+------------+------------+------+---------+---------+-----------------+----------------------------+\n|   chatglm2-6b   |    llm     | 172.17.0.2 | 8001 |   True  |   True  |                 | 2023-09-12T23:04:31.287654 |\n|  WorkerManager  |  service   | 172.17.0.2 | 8001 |   True  |   True  |                 | 2023-09-12T23:04:31.286668 |\n|  WorkerManager  |  service   | 172.17.0.2 | 8003 |   True  |   True  |                 | 2023-09-12T23:04:29.845617 |\n|  WorkerManager  |  service   | 172.17.0.2 | 8002 |   True  |   True  |                 | 2023-09-12T23:04:24.598439 |\n|     text2vec    |  text2vec  | 172.17.0.2 | 8003 |   True  |   True  |                 | 2023-09-12T23:04:29.844796 |\n| vicuna-13b-v1.5 |    llm     | 172.17.0.2 | 8002 |   True  |   True  |                 | 2023-09-12T23:04:24.597775 |\n+-----------------+------------+------------+------+---------+---------+-----------------+----------------------------+\n"})}),"\n",(0,o.jsx)(n.h2,{id:"use-model-serving",children:"Use model serving"}),"\n",(0,o.jsxs)(n.p,{children:["The model service deployed as above can be used through dbgpt_server. First modify the ",(0,o.jsx)(n.code,{children:".env"})," configuration file to change the connection model address"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"dbgpt start webserver --light\n"})}),"\n",(0,o.jsx)(n.h2,{id:"start-webserver",children:"Start Webserver"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"LLM_MODEL=vicuna-13b-v1.5\n# The current default MODEL_SERVER address is the address of the Model Controller\nMODEL_SERVER=http://127.0.0.1:8000\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"--light"})," means not to start the embedded model service."]}),"\n",(0,o.jsx)(n.p,{children:"Or it can be started directly by command to formulate the model."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"LLM_MODEL=chatglm2-6b dbgpt start webserver --light\n"})}),"\n",(0,o.jsx)(n.h2,{id:"command-line-usage",children:"Command line usage"}),"\n",(0,o.jsx)(n.p,{children:"For more information about the use of the command line, you can view the command line help. The following is a reference example."}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["View dbgpt help ",(0,o.jsx)(n.code,{children:"dbgpt --help"})]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"dbgpt --help\n\nAlready connect 'dbgpt'\nUsage: dbgpt [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --log-level TEXT  Log level\n  --version         Show the version and exit.\n  --help            Show this message and exit.\n\nCommands:\n  install    Install dependencies, plugins, etc.\n  knowledge  Knowledge command line tool\n  model      Clients that manage model serving\n  start      Start specific server.\n  stop       Start specific server.\n  trace      Analyze and visualize trace spans.\n"})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["Check the dbgpt start command ",(0,o.jsx)(n.code,{children:"dbgpt start --help"})]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"dbgpt start --help\n\nAlready connect 'dbgpt'\nUsage: dbgpt start [OPTIONS] COMMAND [ARGS]...\n\n  Start specific server.\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  apiserver   Start apiserver\n  controller  Start model controller\n  webserver   Start webserver(dbgpt_server.py)\n  worker      Start model worker\n(dbgpt_env) magic@B-4TMH9N3X-2120 ~ %\n"})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["View the dbgpt start model service help command ",(0,o.jsx)(n.code,{children:"dbgpt start worker --help"})]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"dbgpt start worker --help\n\nAlready connect 'dbgpt'\nUsage: dbgpt start worker [OPTIONS]\n\n  Start model worker\n\nOptions:\n  --model_name TEXT               Model name  [required]\n  --model_path TEXT               Model path  [required]\n  --worker_type TEXT              Worker type\n  --worker_class TEXT             Model worker class,\n                                  pilot.model.cluster.DefaultModelWorker\n  --model_type TEXT               Model type: huggingface, llama.cpp, proxy\n                                  and vllm  [default: huggingface]\n  --host TEXT                     Model worker deploy host  [default: 0.0.0.0]\n  --port INTEGER                  Model worker deploy port  [default: 8001]\n  --daemon                        Run Model Worker in background\n  --limit_model_concurrency INTEGER\n                                  Model concurrency limit  [default: 5]\n  --standalone                    Standalone mode. If True, embedded Run\n                                  ModelController\n  --register                      Register current worker to model controller\n                                  [default: True]\n  --worker_register_host TEXT     The ip address of current worker to register\n                                  to ModelController. If None, the address is\n                                  automatically determined\n  --controller_addr TEXT          The Model controller address to register\n  --send_heartbeat                Send heartbeat to model controller\n                                  [default: True]\n  --heartbeat_interval INTEGER    The interval for sending heartbeats\n                                  (seconds)  [default: 20]\n  --log_level TEXT                Logging level\n  --log_file TEXT                 The filename to store log  [default:\n                                  dbgpt_model_worker_manager.log]\n  --tracer_file TEXT              The filename to store tracer span records\n                                  [default:\n                                  dbgpt_model_worker_manager_tracer.jsonl]\n  --tracer_storage_cls TEXT       The storage class to storage tracer span\n                                  records\n  --device TEXT                   Device to run model. If None, the device is\n                                  automatically determined\n  --prompt_template TEXT          Prompt template. If None, the prompt\n                                  template is automatically determined from\n                                  model path, supported template: zero_shot,vi\n                                  cuna_v1.1,llama-2,codellama,alpaca,baichuan-\n                                  chat,internlm-chat\n  --max_context_size INTEGER      Maximum context size  [default: 4096]\n  --num_gpus INTEGER              The number of gpus you expect to use, if it\n                                  is empty, use all of them as much as\n                                  possible\n  --max_gpu_memory TEXT           The maximum memory limit of each GPU, only\n                                  valid in multi-GPU configuration\n  --cpu_offloading                CPU offloading\n  --load_8bit                     8-bit quantization\n  --load_4bit                     4-bit quantization\n  --quant_type TEXT               Quantization datatypes, `fp4` (four bit\n                                  float) and `nf4` (normal four bit float),\n                                  only valid when load_4bit=True  [default:\n                                  nf4]\n  --use_double_quant              Nested quantization, only valid when\n                                  load_4bit=True  [default: True]\n  --compute_dtype TEXT            Model compute type\n  --trust_remote_code             Trust remote code  [default: True]\n  --verbose                       Show verbose output.\n  --help                          Show this message and exit.\n"})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["View dbgpt model service related commands ",(0,o.jsx)(n.code,{children:"dbgpt model --help"})]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"dbgpt model --help\n\n\nAlready connect 'dbgpt'\nUsage: dbgpt model [OPTIONS] COMMAND [ARGS]...\n\n  Clients that manage model serving\n\nOptions:\n  --address TEXT  Address of the Model Controller to connect to. Just support\n                  light deploy model, If the environment variable\n                  CONTROLLER_ADDRESS is configured, read from the environment\n                  variable\n  --help          Show this message and exit.\n\nCommands:\n  chat     Interact with your bot from the command line\n  list     List model instances\n  restart  Restart model instances\n  start    Start model instances\n  stop     Stop model instances\n"})})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>d});var o=t(6540);const r={},l=o.createContext(r);function a(e){const n=o.useContext(l);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(l.Provider,{value:n},e.children)}}}]);