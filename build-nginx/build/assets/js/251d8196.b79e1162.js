"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6831],{2154:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>p,frontMatter:()=>l,metadata:()=>s,toc:()=>c});var i=t(4848),a=t(8453);const l={},o="OpenAI SDK Calls Local Multi-model",s={id:"installation/advanced_usage/OpenAI_SDK_call",title:"OpenAI SDK Calls Local Multi-model",description:"The call of multi-model services is compatible with the OpenAI interface, and the models deployed in DB-GPT can be directly called through the OpenAI SDK.",source:"@site/versioned_docs/version-v0.5.1/installation/advanced_usage/OpenAI_SDK_call.md",sourceDirName:"installation/advanced_usage",slug:"/installation/advanced_usage/OpenAI_SDK_call",permalink:"/docs/v0.5.1/installation/advanced_usage/OpenAI_SDK_call",draft:!1,unlisted:!1,tags:[],version:"v0.5.1",frontMatter:{},sidebar:"docs",previous:{title:"vLLM Inference",permalink:"/docs/v0.5.1/installation/advanced_usage/vLLM_inference"},next:{title:"Application",permalink:"/docs/v0.5.1/application"}},r={},c=[{value:"Start apiserver",id:"start-apiserver",level:2},{value:"Verify",id:"verify",level:2},{value:"cURL validation",id:"curl-validation",level:3},{value:"Verify via OpenAI SDK",id:"verify-via-openai-sdk",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"openai-sdk-calls-local-multi-model",children:"OpenAI SDK Calls Local Multi-model"}),"\n",(0,i.jsx)(n.p,{children:"The call of multi-model services is compatible with the OpenAI interface, and the models deployed in DB-GPT can be directly called through the OpenAI SDK."}),"\n",(0,i.jsx)(n.admonition,{title:"note",type:"info",children:(0,i.jsxs)(n.p,{children:["\u26a0\ufe0f Before using this project, you must first deploy the model service, which can be deployed through the ",(0,i.jsx)(n.a,{href:"/docs/installation/model_service/cluster",children:"cluster deployment tutorial"}),"."]})}),"\n",(0,i.jsx)(n.h2,{id:"start-apiserver",children:"Start apiserver"}),"\n",(0,i.jsxs)(n.p,{children:["After deploying the model service, you need to start the API Server. By default, the model API Server uses port ",(0,i.jsx)(n.code,{children:"8100"})," to start."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"dbgpt start apiserver --controller_addr http://127.0.0.1:8000 --api_keys EMPTY\n\n"})}),"\n",(0,i.jsx)(n.h2,{id:"verify",children:"Verify"}),"\n",(0,i.jsx)(n.h3,{id:"curl-validation",children:"cURL validation"}),"\n",(0,i.jsx)(n.p,{children:"After the apiserver is started, the service call can be verified. First, let's look at verification through curl."}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"List models"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:8100/api/v1/models \\\n-H "Authorization: Bearer EMPTY" \\\n-H "Content-Type: application/json"\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"Chat"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:8100/api/v1/chat/completions \\\n-H "Authorization: Bearer EMPTY" \\\n-H "Content-Type: application/json" \\\n-d \'{"model": "vicuna-13b-v1.5", "messages": [{"role": "user", "content": "hello"}]}\'\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"Embedding"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:8100/api/v1/embeddings \\\n-H "Authorization: Bearer EMPTY" \\\n-H "Content-Type: application/json" \\\n-d \'{\n    "model": "text2vec",\n    "input": "Hello world!"\n}\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"verify-via-openai-sdk",children:"Verify via OpenAI SDK"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'import openai\nopenai.api_key = "EMPTY"\nopenai.api_base = "http://127.0.0.1:8100/api/v1"\nmodel = "vicuna-13b-v1.5"\n\ncompletion = openai.ChatCompletion.create(\n  model=model,\n  messages=[{"role": "user", "content": "hello"}]\n)\n# print the completion\nprint(completion.choices[0].message.content)\n'})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var i=t(6540);const a={},l=i.createContext(a);function o(e){const n=i.useContext(l);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);