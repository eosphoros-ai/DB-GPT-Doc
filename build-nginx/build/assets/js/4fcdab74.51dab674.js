"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8056],{3494:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>d,toc:()=>h});var t=l(4848),i=l(8453),a=l(1470),r=l(9365);const s={},o="Source Code Deployment",d={id:"installation/sourcecode",title:"Source Code Deployment",description:"Environmental requirements",source:"@site/versioned_docs/version-v0.5.0/installation/sourcecode.md",sourceDirName:"installation",slug:"/installation/sourcecode",permalink:"/docs/v0.5.0/installation/sourcecode",draft:!1,unlisted:!1,tags:[],version:"v0.5.0",frontMatter:{},sidebar:"docs",previous:{title:"Installation",permalink:"/docs/v0.5.0/installation"},next:{title:"Docker Deployment",permalink:"/docs/v0.5.0/installation/docker"}},c={},h=[{value:"Environmental requirements",id:"environmental-requirements",level:2},{value:"Download source code",id:"download-source-code",level:3},{value:"Miniconda environment installation",id:"miniconda-environment-installation",level:3},{value:"Model deployment",id:"model-deployment",level:2},{value:"Proxy model",id:"proxy-model",level:3},{value:"Local model",id:"local-model",level:3},{value:"Hardware requirements description",id:"hardware-requirements-description",level:5},{value:"Download LLM",id:"download-llm",level:5},{value:"Environment variable configuration, configure the LLM_MODEL parameter in the <code>.env</code> file",id:"environment-variable-configuration-configure-the-llm_model-parameter-in-the-env-file",level:5},{value:"Hardware requirements description",id:"hardware-requirements-description-1",level:5},{value:"Download LLM",id:"download-llm-1",level:5},{value:"Environment variable configuration, configure the LLM_MODEL parameter in the <code>.env</code> file",id:"environment-variable-configuration-configure-the-llm_model-parameter-in-the-env-file-1",level:5},{value:"Hardware requirements description",id:"hardware-requirements-description-2",level:5},{value:"Download LLM",id:"download-llm-2",level:5},{value:"Environment variable configuration, configure the LLM_MODEL parameter in the <code>.env</code> file",id:"environment-variable-configuration-configure-the-llm_model-parameter-in-the-env-file-2",level:5},{value:"llama.cpp(CPU)",id:"llamacppcpu",level:3},{value:"Document preparation",id:"document-preparation",level:4},{value:"Install dependencies",id:"install-dependencies",level:4},{value:"Modify configuration file",id:"modify-configuration-file",level:4},{value:"More descriptions",id:"more-descriptions",level:4},{value:"Install DB-GPT Application Database",id:"install-db-gpt-application-database",level:2},{value:"Test data (optional)",id:"test-data-optional",level:2},{value:"Run service",id:"run-service",level:2},{value:"Run service",id:"run-service-1",level:3},{value:"Visit website",id:"visit-website",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"source-code-deployment",children:"Source Code Deployment"}),"\n",(0,t.jsx)(n.h2,{id:"environmental-requirements",children:"Environmental requirements"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{style:{textAlign:"center"},children:"Startup Mode"}),(0,t.jsx)(n.th,{style:{textAlign:"center"},children:"CPU * MEM"}),(0,t.jsx)(n.th,{style:{textAlign:"center"},children:"GPU"}),(0,t.jsx)(n.th,{style:{textAlign:"center"},children:"Remark"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"Proxy model"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"4C * 8G"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"None"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"Proxy model does not rely on GPU"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"Local model"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"8C * 32G"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"24G"}),(0,t.jsx)(n.td,{style:{textAlign:"center"},children:"It is best to start locally with a GPU of 24G or above"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"download-source-code",children:"Download source code"}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Download DB-GPT"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"git clone https://github.com/eosphoros-ai/DB-GPT.git\n"})}),"\n",(0,t.jsx)(n.h3,{id:"miniconda-environment-installation",children:"Miniconda environment installation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The default database uses SQLite, so there is no need to install a database in the default startup mode. If you need to use other databases, you can read the ",(0,t.jsx)(n.a,{href:"/docs/application_manual/advanced_tutorial",children:"advanced tutorials"})," below. We recommend installing the Python virtual environment through the conda virtual environment. For the installation of Miniconda environment, please refer to the ",(0,t.jsx)(n.a,{href:"https://docs.conda.io/projects/miniconda/en/latest/",children:"Miniconda installation tutorial"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Create a Python virtual environment"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'python >= 3.10\nconda create -n dbgpt_env python=3.10\nconda activate dbgpt_env\n\n# it will take some minutes\npip install -e ".[default]"\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Copy environment variables"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cp .env.template  .env\n"})}),"\n",(0,t.jsx)(n.h2,{id:"model-deployment",children:"Model deployment"}),"\n",(0,t.jsx)(n.p,{children:"DB-GPT can be deployed on servers with lower hardware through proxy model, or as a private local model under the GPU environment. If your hardware configuration is low, you can use third-party large language model API services, such as OpenAI, Azure, Qwen, ERNIE Bot, etc."}),"\n",(0,t.jsxs)(n.admonition,{title:"note",type:"info",children:[(0,t.jsx)(n.p,{children:"\u26a0\ufe0f  You need to ensure that git-lfs is installed"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"\u25cf CentOS installation: yum install git-lfs\n\u25cf Ubuntu installation: apt-get install git-lfs\n\u25cf MacOS installation: brew install git-lfs\n"})})]}),"\n",(0,t.jsx)(n.h3,{id:"proxy-model",children:"Proxy model"}),"\n","\n","\n",(0,t.jsxs)(a.A,{defaultValue:"openai",values:[{label:"Open AI",value:"openai"},{label:"Qwen",value:"qwen"},{label:"ChatGLM",value:"chatglm"},{label:"WenXin",value:"erniebot"}],children:[(0,t.jsxs)(r.A,{value:"openai",label:"open ai",children:[(0,t.jsx)(n.p,{children:"Install dependencies"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pip install  -e ".[openai]"\n'})}),(0,t.jsx)(n.p,{children:"Download embedding model"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cd DB-GPT\nmkdir models and cd models\ngit clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\n"})}),(0,t.jsxs)(n.p,{children:["Configure the proxy and modify LLM_MODEL, PROXY_API_URL and API_KEY in the ",(0,t.jsx)(n.code,{children:".env"}),"file"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# .env\nLLM_MODEL=chatgpt_proxyllm\nPROXY_API_KEY={your-openai-sk}\nPROXY_SERVER_URL=https://api.openai.com/v1/chat/completions\n# If you use gpt-4\n# PROXYLLM_BACKEND=gpt-4\n"})})]}),(0,t.jsxs)(r.A,{value:"qwen",label:"\u901a\u4e49\u5343\u95ee",children:[(0,t.jsx)(n.p,{children:"Install dependencies"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pip install dashscope\n"})}),(0,t.jsx)(n.p,{children:"Download embedding model"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cd DB-GPT\nmkdir models and cd models\n\n# embedding model\ngit clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\nor\ngit clone https://huggingface.co/moka-ai/m3e-large\n"})}),(0,t.jsxs)(n.p,{children:["Configure the proxy and modify LLM_MODEL, PROXY_API_URL and API_KEY in the ",(0,t.jsx)(n.code,{children:".env"}),"file"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# .env\n# Aliyun tongyiqianwen\nLLM_MODEL=tongyi_proxyllm\nTONGYI_PROXY_API_KEY={your-tongyi-sk}\nPROXY_SERVER_URL={your_service_url}\n"})})]}),(0,t.jsxs)(r.A,{value:"chatglm",label:"chatglm",children:[(0,t.jsx)(n.p,{children:"Install dependencies"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pip install zhipuai\n"})}),(0,t.jsx)(n.p,{children:"Download embedding model"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cd DB-GPT\nmkdir models and cd models\n\n# embedding model\ngit clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\nor\ngit clone https://huggingface.co/moka-ai/m3e-large\n"})}),(0,t.jsxs)(n.p,{children:["Configure the proxy and modify LLM_MODEL, PROXY_API_URL and API_KEY in the ",(0,t.jsx)(n.code,{children:".env"}),"file"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# .env\nLLM_MODEL=zhipu_proxyllm\nPROXY_SERVER_URL={your_service_url}\nZHIPU_MODEL_VERSION={version}\nZHIPU_PROXY_API_KEY={your-zhipu-sk}\n"})})]}),(0,t.jsxs)(r.A,{value:"erniebot",label:"\u6587\u5fc3\u4e00\u8a00",default:!0,children:[(0,t.jsx)(n.p,{children:"Download embedding model"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cd DB-GPT\nmkdir models and cd models\n\n# embedding model\ngit clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\nor\ngit clone https://huggingface.co/moka-ai/m3e-large\n"})}),(0,t.jsxs)(n.p,{children:["Configure the proxy and modify LLM_MODEL, MODEL_VERSION, API_KEY and API_SECRET in the ",(0,t.jsx)(n.code,{children:".env"}),"file"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# .env\nLLM_MODEL=wenxin_proxyllm\nWEN_XIN_MODEL_VERSION={version} # ERNIE-Bot or ERNIE-Bot-turbo\nWEN_XIN_API_KEY={your-wenxin-sk}\nWEN_XIN_API_SECRET={your-wenxin-sct}\n"})})]})]}),"\n",(0,t.jsx)(n.admonition,{title:"note",type:"info",children:(0,t.jsxs)(n.p,{children:["\u26a0\ufe0f Be careful not to overwrite the contents of the ",(0,t.jsx)(n.code,{children:".env"})," configuration file"]})}),"\n",(0,t.jsx)(n.h3,{id:"local-model",children:"Local model"}),"\n",(0,t.jsxs)(a.A,{defaultValue:"vicuna",values:[{label:"Vicuna",value:"vicuna"},{label:"Baichuan",value:"baichuan"},{label:"ChatGLM",value:"chatglm"}],children:[(0,t.jsxs)(r.A,{value:"vicuna",label:"vicuna",children:[(0,t.jsx)(n.h5,{id:"hardware-requirements-description",children:"Hardware requirements description"}),(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Quantize"}),(0,t.jsx)(n.th,{children:"VRAM Size"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Vicuna-7b-1.5"}),(0,t.jsx)(n.td,{children:"4-bit"}),(0,t.jsx)(n.td,{children:"8GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Vicuna-7b-1.5"}),(0,t.jsx)(n.td,{children:"8-bit"}),(0,t.jsx)(n.td,{children:"12GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Vicuna-13b-v1.5"}),(0,t.jsx)(n.td,{children:"4-bit"}),(0,t.jsx)(n.td,{children:"12GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Vicuna-13b-v1.5"}),(0,t.jsx)(n.td,{children:"8-bit"}),(0,t.jsx)(n.td,{children:"24GB"})]})]})]}),(0,t.jsx)(n.h5,{id:"download-llm",children:"Download LLM"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cd DB-GPT\nmkdir models and cd models\n\n# embedding model\ngit clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\nor\ngit clone https://huggingface.co/moka-ai/m3e-large\n\n# llm model, if you use openai or Azure or tongyi llm api service, you don't need to download llm model\ngit clone https://huggingface.co/lmsys/vicuna-13b-v1.5\n\n"})}),(0,t.jsxs)(n.h5,{id:"environment-variable-configuration-configure-the-llm_model-parameter-in-the-env-file",children:["Environment variable configuration, configure the LLM_MODEL parameter in the ",(0,t.jsx)(n.code,{children:".env"})," file"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# .env\nLLM_MODEL=vicuna-13b-v1.5\n"})})]}),(0,t.jsxs)(r.A,{value:"baichuan",label:"baichuan",children:[(0,t.jsx)(n.h5,{id:"hardware-requirements-description-1",children:"Hardware requirements description"}),(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Quantize"}),(0,t.jsx)(n.th,{children:"VRAM Size"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Baichuan-7b"}),(0,t.jsx)(n.td,{children:"4-bit"}),(0,t.jsx)(n.td,{children:"8GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Baichuan-7b"}),(0,t.jsx)(n.td,{children:"8-bit"}),(0,t.jsx)(n.td,{children:"12GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Baichuan-13b"}),(0,t.jsx)(n.td,{children:"4-bit"}),(0,t.jsx)(n.td,{children:"12GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Baichuan-13b"}),(0,t.jsx)(n.td,{children:"8-bit"}),(0,t.jsx)(n.td,{children:"20GB"})]})]})]}),(0,t.jsx)(n.h5,{id:"download-llm-1",children:"Download LLM"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cd DB-GPT\nmkdir models and cd models\n\n# embedding model\ngit clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\nor\ngit clone https://huggingface.co/moka-ai/m3e-large\n\n# llm model\ngit clone https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat\nor\ngit clone https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat\n\n"})}),(0,t.jsxs)(n.h5,{id:"environment-variable-configuration-configure-the-llm_model-parameter-in-the-env-file-1",children:["Environment variable configuration, configure the LLM_MODEL parameter in the ",(0,t.jsx)(n.code,{children:".env"})," file"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# .env\nLLM_MODEL=baichuan2-13b\n"})})]}),(0,t.jsxs)(r.A,{value:"chatglm",label:"chatglm",children:[(0,t.jsx)(n.h5,{id:"hardware-requirements-description-2",children:"Hardware requirements description"}),(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Quantize"}),(0,t.jsx)(n.th,{children:"VRAM Size"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"ChatGLM-6b"}),(0,t.jsx)(n.td,{children:"4-bit"}),(0,t.jsx)(n.td,{children:"7GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"ChatGLM-6b"}),(0,t.jsx)(n.td,{children:"8-bit"}),(0,t.jsx)(n.td,{children:"9GB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"ChatGLM-6b"}),(0,t.jsx)(n.td,{children:"FP16"}),(0,t.jsx)(n.td,{children:"14GB"})]})]})]}),(0,t.jsx)(n.h5,{id:"download-llm-2",children:"Download LLM"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cd DB-GPT\nmkdir models and cd models\n\n# embedding model\ngit clone https://huggingface.co/GanymedeNil/text2vec-large-chinese\nor\ngit clone https://huggingface.co/moka-ai/m3e-large\n\n# llm model\ngit clone https://huggingface.co/THUDM/chatglm2-6b\n\n"})}),(0,t.jsxs)(n.h5,{id:"environment-variable-configuration-configure-the-llm_model-parameter-in-the-env-file-2",children:["Environment variable configuration, configure the LLM_MODEL parameter in the ",(0,t.jsx)(n.code,{children:".env"})," file"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# .env\nLLM_MODEL=chatglm2-6b\n"})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"llamacppcpu",children:"llama.cpp(CPU)"}),"\n",(0,t.jsx)(n.admonition,{title:"note",type:"info",children:(0,t.jsx)(n.p,{children:"\u26a0\ufe0f llama.cpp can be run on Mac M1 or Mac M2"})}),"\n",(0,t.jsx)(n.p,{children:"DB-GPT also supports the lower-cost inference framework llama.cpp, which can be used through llama-cpp-python."}),"\n",(0,t.jsx)(n.h4,{id:"document-preparation",children:"Document preparation"}),"\n",(0,t.jsx)(n.p,{children:"Before using llama.cpp, you first need to prepare the model file in gguf format. There are two ways to obtain it. You can choose one method to obtain the corresponding file."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Method 1: Download the converted model"})}),"\n",(0,t.jsxs)(n.p,{children:["If you want to use ",(0,t.jsx)(n.a,{href:"https://huggingface.co/lmsys/vicuna-13b-v1.5",children:"Vicuna-13b-v1.5"}),", you can download the converted file ",(0,t.jsx)(n.a,{href:"https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGUF",children:"TheBloke/vicuna-13B-v1.5-GGUF"}),", only this one file is needed. Download the file and put it in the model path. You need to rename the model to: ",(0,t.jsx)(n.code,{children:"ggml-model-q4_0.gguf"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"wget https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGUF/resolve/main/vicuna-13b-v1.5.Q4_K_M.gguf -O models/ggml-model-q4_0.gguf\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Method 2: Convert files yourself"})}),"\n",(0,t.jsxs)(n.p,{children:["During use, you can also convert the model file yourself according to the instructions in ",(0,t.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp#prepare-data--run",children:"llama.cpp#prepare-data\u2013run"}),", and place the converted file in the models directory and name it ",(0,t.jsx)(n.code,{children:"ggml-model-q4_0.gguf"}),"."]}),"\n",(0,t.jsx)(n.h4,{id:"install-dependencies",children:"Install dependencies"}),"\n",(0,t.jsx)(n.p,{children:"llama.cpp is an optional installation item in DB-GPT. You can install it with the following command."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pip install -e ".[llama_cpp]"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"modify-configuration-file",children:"Modify configuration file"}),"\n",(0,t.jsxs)(n.p,{children:["Modify the ",(0,t.jsx)(n.code,{children:".env"})," file to use llama.cpp, and then you can start the service by running the ",(0,t.jsx)(n.a,{href:"/docs/quickstart.mdx",children:"command"})]}),"\n",(0,t.jsx)(n.h4,{id:"more-descriptions",children:"More descriptions"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"environment variables"}),(0,t.jsx)(n.th,{children:"default value"}),(0,t.jsx)(n.th,{children:"description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_prompt_template"})}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["Prompt template now supports ",(0,t.jsx)(n.code,{children:"zero_shot, vicuna_v1.1,alpaca,llama-2,baichuan-chat,internlm-chat"}),". If it is None, the model Prompt template can be automatically obtained according to the model path."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_model_path"})}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsx)(n.td,{children:"model path"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_n_gpu_layers"})}),(0,t.jsx)(n.td,{children:"1000000000"}),(0,t.jsx)(n.td,{children:"How many network layers to transfer to the GPU, set this to 1000000000 to transfer all layers to the GPU. If your GPU is low on memory, you can set a lower number, for example: 10."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_n_threads"})}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsx)(n.td,{children:"The number of threads to use. If None, the number of threads will be determined automatically."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_n_batch"})}),(0,t.jsx)(n.td,{children:"512"}),(0,t.jsx)(n.td,{children:"The maximum number of prompt tokens to be batched together when calling llama_eval"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_n_gqa"})}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsx)(n.td,{children:"For the llama-2 70B model, Grouped-query attention must be 8."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_rms_norm_eps"})}),(0,t.jsx)(n.td,{children:"5e-06"}),(0,t.jsx)(n.td,{children:"For the llama-2 model, 5e-6 is a good value."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_cache_capacity"})}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsx)(n.td,{children:"Maximum model cache size. For example: 2000MiB, 2GiB"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"llama_cpp_prefer_cpu"})}),(0,t.jsx)(n.td,{children:"False"}),(0,t.jsx)(n.td,{children:"If a GPU is available, the GPU will be used first by default unless prefer_cpu=False is configured."})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"install-db-gpt-application-database",children:"Install DB-GPT Application Database"}),"\n",(0,t.jsxs)(a.A,{defaultValue:"sqlite",values:[{label:"SQLite",value:"sqlite"},{label:"MySQL",value:"mysql"}],children:[(0,t.jsx)(r.A,{value:"sqlite",label:"sqlite",children:(0,t.jsx)(n.admonition,{title:"NOTE",type:"tip",children:(0,t.jsx)(n.p,{children:"You do not need to separately create the database tables related to the DB-GPT application in SQLite;\nthey will be created automatically for you by default."})})}),(0,t.jsxs)(r.A,{value:"mysql",label:"MySQL",children:[(0,t.jsx)(n.admonition,{title:"NOTE",type:"warning",children:(0,t.jsx)(n.p,{children:"After version 0.4.7, we removed the automatic generation of MySQL database Schema for safety."})}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Frist, execute MySQL script to create database and tables."}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"$ mysql -h127.0.0.1 -uroot -p{your_password} < ./assets/schema/dbgpt.sql\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["Second, set DB-GPT MySQL database settings in ",(0,t.jsx)(n.code,{children:".env"})," file."]}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"LOCAL_DB_TYPE=mysql\nLOCAL_DB_USER= {your username}\nLOCAL_DB_PASSWORD={your_password}\nLOCAL_DB_HOST=127.0.0.1\nLOCAL_DB_PORT=3306\n"})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"test-data-optional",children:"Test data (optional)"}),"\n",(0,t.jsx)(n.p,{children:"The DB-GPT project has a part of test data built-in by default, which can be loaded into the local database for testing through the following command"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Linux"})}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"bash ./scripts/examples/load_examples.sh\n\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Windows"})}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:".\\scripts\\examples\\load_examples.bat\n"})}),"\n",(0,t.jsx)(n.h2,{id:"run-service",children:"Run service"}),"\n",(0,t.jsx)(n.p,{children:"The DB-GPT service is packaged into a server, and the entire DB-GPT service can be started through the following command."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"python dbgpt/app/dbgpt_server.py\n"})}),"\n",(0,t.jsxs)(n.admonition,{title:"NOTE",type:"info",children:[(0,t.jsx)(n.h3,{id:"run-service-1",children:"Run service"}),(0,t.jsx)(n.p,{children:"If you are running version v0.4.3 or earlier, please start with the following command:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"python pilot/server/dbgpt_server.py\n"})})]}),"\n",(0,t.jsx)(n.h2,{id:"visit-website",children:"Visit website"}),"\n",(0,t.jsxs)(n.p,{children:["Open the browser and visit ",(0,t.jsx)(n.a,{href:"http://localhost:5000",children:(0,t.jsx)(n.code,{children:"http://localhost:5000"})})]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},9365:(e,n,l)=>{l.d(n,{A:()=>r});l(6540);var t=l(53);const i={tabItem:"tabItem_Ymn6"};var a=l(4848);function r(e){let{children:n,hidden:l,className:r}=e;return(0,a.jsx)("div",{role:"tabpanel",className:(0,t.A)(i.tabItem,r),hidden:l,children:n})}},1470:(e,n,l)=>{l.d(n,{A:()=>_});var t=l(6540),i=l(53),a=l(3104),r=l(6347),s=l(205),o=l(7485),d=l(1682),c=l(9466);function h(e){return t.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:l}=e;return(0,t.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:l,attributes:t,default:i}}=e;return{value:n,label:l,attributes:t,default:i}}))}(l);return function(e){const n=(0,d.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,l])}function p(e){let{value:n,tabValues:l}=e;return l.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:l}=e;const i=(0,r.W6)(),a=function(e){let{queryString:n=!1,groupId:l}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!l)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return l??null}({queryString:n,groupId:l});return[(0,o.aZ)(a),(0,t.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(i.location.search);n.set(a,e),i.replace({...i.location,search:n.toString()})}),[a,i])]}function x(e){const{defaultValue:n,queryString:l=!1,groupId:i}=e,a=u(e),[r,o]=(0,t.useState)((()=>function(e){let{defaultValue:n,tabValues:l}=e;if(0===l.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:l}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${l.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=l.find((e=>e.default))??l[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a}))),[d,h]=m({queryString:l,groupId:i}),[x,g]=function(e){let{groupId:n}=e;const l=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,a]=(0,c.Dv)(l);return[i,(0,t.useCallback)((e=>{l&&a.set(e)}),[l,a])]}({groupId:i}),j=(()=>{const e=d??x;return p({value:e,tabValues:a})?e:null})();(0,s.A)((()=>{j&&o(j)}),[j]);return{selectedValue:r,selectValue:(0,t.useCallback)((e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);o(e),h(e),g(e)}),[h,g,a]),tabValues:a}}var g=l(2303);const j={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=l(4848);function f(e){let{className:n,block:l,selectedValue:t,selectValue:r,tabValues:s}=e;const o=[],{blockElementScrollPositionUntilNextRender:d}=(0,a.a_)(),c=e=>{const n=e.currentTarget,l=o.indexOf(n),i=s[l].value;i!==t&&(d(n),r(i))},h=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const l=o.indexOf(e.currentTarget)+1;n=o[l]??o[0];break}case"ArrowLeft":{const l=o.indexOf(e.currentTarget)-1;n=o[l]??o[o.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":l},n),children:s.map((e=>{let{value:n,label:l,attributes:a}=e;return(0,v.jsx)("li",{role:"tab",tabIndex:t===n?0:-1,"aria-selected":t===n,ref:e=>o.push(e),onKeyDown:h,onClick:c,...a,className:(0,i.A)("tabs__item",j.tabItem,a?.className,{"tabs__item--active":t===n}),children:l??n},n)}))})}function b(e){let{lazy:n,children:l,selectedValue:i}=e;const a=(Array.isArray(l)?l:[l]).filter(Boolean);if(n){const e=a.find((e=>e.props.value===i));return e?(0,t.cloneElement)(e,{className:"margin-top--md"}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:a.map(((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function y(e){const n=x(e);return(0,v.jsxs)("div",{className:(0,i.A)("tabs-container",j.tabList),children:[(0,v.jsx)(f,{...e,...n}),(0,v.jsx)(b,{...e,...n})]})}function _(e){const n=(0,g.A)();return(0,v.jsx)(y,{...e,children:h(e.children)},String(n))}},8453:(e,n,l)=>{l.d(n,{R:()=>r,x:()=>s});var t=l(6540);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);