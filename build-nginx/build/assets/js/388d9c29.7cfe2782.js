"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2749],{4979:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var i=t(4848),s=t(8453);const o={},l="vLLM Inference",a={id:"installation/advanced_usage/vLLM_inference",title:"vLLM Inference",description:"DB-GPT supports vLLM inference, a fast and easy-to-use LLM inference and service library.",source:"@site/versioned_docs/version-v0.5.1/installation/advanced_usage/vLLM_inference.md",sourceDirName:"installation/advanced_usage",slug:"/installation/advanced_usage/vLLM_inference",permalink:"/docs/v0.5.1/installation/advanced_usage/vLLM_inference",draft:!1,unlisted:!1,tags:[],version:"v0.5.1",frontMatter:{},sidebar:"docs",previous:{title:"ProxyLLMs",permalink:"/docs/v0.5.1/installation/advanced_usage/More_proxyllms"},next:{title:"OpenAI SDK Calls Local Multi-model",permalink:"/docs/v0.5.1/installation/advanced_usage/OpenAI_SDK_call"}},r={},c=[{value:"Install dependencies",id:"install-dependencies",level:2},{value:"Modify configuration file",id:"modify-configuration-file",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"vllm-inference",children:"vLLM Inference"}),"\n",(0,i.jsxs)(n.p,{children:["DB-GPT supports ",(0,i.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," inference, a fast and easy-to-use LLM inference and service library."]}),"\n",(0,i.jsx)(n.h2,{id:"install-dependencies",children:"Install dependencies"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"vLLM"})," is an optional dependency in DB-GPT. You can install it manually through the following command."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'$ pip install -e ".[vllm]"\n'})}),"\n",(0,i.jsx)(n.h2,{id:"modify-configuration-file",children:"Modify configuration file"}),"\n",(0,i.jsxs)(n.p,{children:["In the ",(0,i.jsx)(n.code,{children:".env"})," configuration file, modify the inference type of the model to start ",(0,i.jsx)(n.code,{children:"vllm"})," inference."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"LLM_MODEL=vicuna-13b-v1.5\nMODEL_TYPE=vllm\n"})}),"\n",(0,i.jsxs)(n.p,{children:["For more information about the list of models supported by ",(0,i.jsx)(n.code,{children:"vLLM"}),", please refer to the ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models",children:"vLLM supported model document"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function l(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);